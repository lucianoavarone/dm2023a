Arabia_Saudita$title[i] <- title
Arabia_Saudita$date[i] <- date
Arabia_Saudita$text[i] <- body_text
cat(paste0("Link ", i, " of ", nrow(Arabia_Saudita), " processed successfully.\n"))
}, error = function(e) {
cat(paste0("Link ", i, " of ", nrow(Arabia_Saudita), " failed with error: ", e$message, "\n"))
Arabia_Saudita.Mistakes <<- rbind(Arabia_Saudita.Mistakes, data.frame(url = url), stringsAsFactors = FALSE)
})
}
View(Arabia_Saudita)
#Arbol elemental con libreria  rpart
#Debe tener instaladas las librerias  data.table  ,  rpart  y  rpart.plot
#cargo las librerias que necesito
require("data.table")
require("rpart")
require("rpart.plot")
#Aqui se debe poner la carpeta de la materia de SU computadora local
setwd("C:/Users/varon/OneDrive/Escritorio/Data_mining")  #Establezco el Working Directory
#cargo el dataset
dataset  <- fread("./datasets/dataset_pequeno.csv")
dtrain  <- dataset[ foto_mes==202107 ]  #defino donde voy a entrenar
dapply  <- dataset[ foto_mes==202109 ]  #defino donde voy a aplicar el modelo
#genero el modelo,  aqui se construye el arbol
modelo  <- rpart(formula=   "clase_ternaria ~ .",  #quiero predecir clase_ternaria a partir de el resto de las variables
data=      dtrain,  #los datos donde voy a entrenar
xval=      0,
cp=       -1,     #esto significa no limitar la complejidad de los splits
minsplit=  400,     #minima cantidad de registros para que se haga el split
minbucket= 200,     #tamaño minimo de una hoja
maxdepth=  8 )    #profundidad maxima del arbol
#grafico el arbol
prp(modelo, extra=101, digits=-5, branch=1, type=4, varlen=0, faclen=0)
#aplico el modelo a los datos nuevos
prediccion  <- predict( object= modelo,
newdata= dapply,
type = "prob")
#prediccion es una matriz con TRES columnas, llamadas "BAJA+1", "BAJA+2"  y "CONTINUA"
#cada columna es el vector de probabilidades
#agrego a dapply una columna nueva que es la probabilidad de BAJA+2
dapply[ , prob_baja2 := prediccion[, "BAJA+2"] ]
#solo le envio estimulo a los registros con probabilidad de BAJA+2 mayor  a  1/40
dapply[ , Predicted := as.numeric( prob_baja2 > 1/40 ) ]
#genero el archivo para Kaggle
#primero creo la carpeta donde va el experimento
dir.create( "./exp/" )
dir.create( "./exp/KA2001" )
fwrite( dapply[ , list(numero_de_cliente, Predicted) ], #solo los campos para Kaggle
file= "./exp/KA2001/K101_0011.csv",
sep=  "," )
#Arbol elemental con libreria  rpart
#Debe tener instaladas las librerias  data.table  ,  rpart  y  rpart.plot
#cargo las librerias que necesito
require("data.table")
require("rpart")
require("rpart.plot")
#Aqui se debe poner la carpeta de la materia de SU computadora local
setwd("C:/Users/varon/OneDrive/Escritorio/Data_mining")  #Establezco el Working Directory
#cargo el dataset
dataset  <- fread("./datasets/dataset_pequeno.csv")
dtrain  <- dataset[ foto_mes==202107 ]  #defino donde voy a entrenar
dapply  <- dataset[ foto_mes==202109 ]  #defino donde voy a aplicar el modelo
#genero el modelo,  aqui se construye el arbol
modelo  <- rpart(formula=   "clase_ternaria ~ .",  #quiero predecir clase_ternaria a partir de el resto de las variables
data=      dtrain,  #los datos donde voy a entrenar
xval=      0,
cp=       -0.5,     #esto significa no limitar la complejidad de los splits
minsplit=  400,     #minima cantidad de registros para que se haga el split
minbucket= 200,     #tamaño minimo de una hoja
maxdepth=  8 )    #profundidad maxima del arbol
#grafico el arbol
prp(modelo, extra=101, digits=-5, branch=1, type=4, varlen=0, faclen=0)
#aplico el modelo a los datos nuevos
prediccion  <- predict( object= modelo,
newdata= dapply,
type = "prob")
#prediccion es una matriz con TRES columnas, llamadas "BAJA+1", "BAJA+2"  y "CONTINUA"
#cada columna es el vector de probabilidades
#agrego a dapply una columna nueva que es la probabilidad de BAJA+2
dapply[ , prob_baja2 := prediccion[, "BAJA+2"] ]
#solo le envio estimulo a los registros con probabilidad de BAJA+2 mayor  a  1/40
dapply[ , Predicted := as.numeric( prob_baja2 > 1/40 ) ]
#genero el archivo para Kaggle
#primero creo la carpeta donde va el experimento
dir.create( "./exp/" )
dir.create( "./exp/KA2001" )
fwrite( dapply[ , list(numero_de_cliente, Predicted) ], #solo los campos para Kaggle
file= "./exp/KA2001/K101_0012.csv",
sep=  "," )
#Arbol elemental con libreria  rpart
#Debe tener instaladas las librerias  data.table  ,  rpart  y  rpart.plot
#cargo las librerias que necesito
require("data.table")
require("rpart")
require("rpart.plot")
#Aqui se debe poner la carpeta de la materia de SU computadora local
setwd("C:/Users/varon/OneDrive/Escritorio/Data_mining")  #Establezco el Working Directory
#cargo el dataset
dataset  <- fread("./datasets/dataset_pequeno.csv")
dtrain  <- dataset[ foto_mes==202107 ]  #defino donde voy a entrenar
dapply  <- dataset[ foto_mes==202109 ]  #defino donde voy a aplicar el modelo
#genero el modelo,  aqui se construye el arbol
modelo  <- rpart(formula=   "clase_ternaria ~ .",  #quiero predecir clase_ternaria a partir de el resto de las variables
data=      dtrain,  #los datos donde voy a entrenar
xval=      0,
cp=       -1,     #esto significa no limitar la complejidad de los splits
minsplit=  600,     #minima cantidad de registros para que se haga el split
minbucket= 150,     #tamaño minimo de una hoja
maxdepth=  6 )    #profundidad maxima del arbol
#grafico el arbol
prp(modelo, extra=101, digits=-5, branch=1, type=4, varlen=0, faclen=0)
#aplico el modelo a los datos nuevos
prediccion  <- predict( object= modelo,
newdata= dapply,
type = "prob")
#prediccion es una matriz con TRES columnas, llamadas "BAJA+1", "BAJA+2"  y "CONTINUA"
#cada columna es el vector de probabilidades
#agrego a dapply una columna nueva que es la probabilidad de BAJA+2
dapply[ , prob_baja2 := prediccion[, "BAJA+2"] ]
#solo le envio estimulo a los registros con probabilidad de BAJA+2 mayor  a  1/40
dapply[ , Predicted := as.numeric( prob_baja2 > 1/40 ) ]
#genero el archivo para Kaggle
#primero creo la carpeta donde va el experimento
dir.create( "./exp/" )
dir.create( "./exp/KA2001" )
fwrite( dapply[ , list(numero_de_cliente, Predicted) ], #solo los campos para Kaggle
file= "./exp/KA2001/K101_0013.csv",
sep=  "," )
#Arbol elemental con libreria  rpart
#Debe tener instaladas las librerias  data.table  ,  rpart  y  rpart.plot
#cargo las librerias que necesito
require("data.table")
require("rpart")
require("rpart.plot")
#Aqui se debe poner la carpeta de la materia de SU computadora local
setwd("C:/Users/varon/OneDrive/Escritorio/Data_mining")  #Establezco el Working Directory
#cargo el dataset
dataset  <- fread("./datasets/dataset_pequeno.csv")
dtrain  <- dataset[ foto_mes==202107 ]  #defino donde voy a entrenar
dapply  <- dataset[ foto_mes==202109 ]  #defino donde voy a aplicar el modelo
#genero el modelo,  aqui se construye el arbol
modelo  <- rpart(formula=   "clase_ternaria ~ .",  #quiero predecir clase_ternaria a partir de el resto de las variables
data=      dtrain,  #los datos donde voy a entrenar
xval=      0,
cp=       -0.5,     #esto significa no limitar la complejidad de los splits
minsplit=  600,     #minima cantidad de registros para que se haga el split
minbucket= 16,     #tamaño minimo de una hoja
maxdepth=  8 )    #profundidad maxima del arbol
#grafico el arbol
prp(modelo, extra=101, digits=-5, branch=1, type=4, varlen=0, faclen=0)
#aplico el modelo a los datos nuevos
prediccion  <- predict( object= modelo,
newdata= dapply,
type = "prob")
#prediccion es una matriz con TRES columnas, llamadas "BAJA+1", "BAJA+2"  y "CONTINUA"
#cada columna es el vector de probabilidades
#agrego a dapply una columna nueva que es la probabilidad de BAJA+2
dapply[ , prob_baja2 := prediccion[, "BAJA+2"] ]
#solo le envio estimulo a los registros con probabilidad de BAJA+2 mayor  a  1/40
dapply[ , Predicted := as.numeric( prob_baja2 > 1/40 ) ]
#genero el archivo para Kaggle
#primero creo la carpeta donde va el experimento
dir.create( "./exp/" )
dir.create( "./exp/KA2001" )
fwrite( dapply[ , list(numero_de_cliente, Predicted) ], #solo los campos para Kaggle
file= "./exp/KA2001/K101_00110.csv",
sep=  "," )
#Arbol elemental con libreria  rpart
#Debe tener instaladas las librerias  data.table  ,  rpart  y  rpart.plot
#cargo las librerias que necesito
require("data.table")
require("rpart")
require("rpart.plot")
#Aqui se debe poner la carpeta de la materia de SU computadora local
setwd("C:/Users/varon/OneDrive/Escritorio/Data_mining")  #Establezco el Working Directory
#cargo el dataset
dataset  <- fread("./datasets/dataset_pequeno.csv")
dtrain  <- dataset[ foto_mes==202107 ]  #defino donde voy a entrenar
dapply  <- dataset[ foto_mes==202109 ]  #defino donde voy a aplicar el modelo
#genero el modelo,  aqui se construye el arbol
modelo  <- rpart(formula=   "clase_ternaria ~ .",  #quiero predecir clase_ternaria a partir de el resto de las variables
data=      dtrain,  #los datos donde voy a entrenar
xval=      0,
cp=       -0.5,     #esto significa no limitar la complejidad de los splits
minsplit=  100,     #minima cantidad de registros para que se haga el split
minbucket= 50,     #tamaño minimo de una hoja
maxdepth=  6 )    #profundidad maxima del arbol
#Arbol elemental con libreria  rpart
#Debe tener instaladas las librerias  data.table  ,  rpart  y  rpart.plot
#cargo las librerias que necesito
require("data.table")
require("rpart")
require("rpart.plot")
#Aqui se debe poner la carpeta de la materia de SU computadora local
setwd("C:/Users/varon/OneDrive/Escritorio/Data_mining")  #Establezco el Working Directory
#cargo el dataset
dataset  <- fread("./datasets/dataset_pequeno.csv")
dtrain  <- dataset[ foto_mes==202107 ]  #defino donde voy a entrenar
dapply  <- dataset[ foto_mes==202109 ]  #defino donde voy a aplicar el modelo
#genero el modelo,  aqui se construye el arbol
modelo  <- rpart(formula=   "clase_ternaria ~ .",  #quiero predecir clase_ternaria a partir de el resto de las variables
data=      dtrain,  #los datos donde voy a entrenar
xval=      0,
cp=       -0.5,     #esto significa no limitar la complejidad de los splits
minsplit=  100,     #minima cantidad de registros para que se haga el split
minbucket= 50,     #tamaño minimo de una hoja
maxdepth=  6 )    #profundidad maxima del arbol
#grafico el arbol
prp(modelo, extra=101, digits=-5, branch=1, type=4, varlen=0, faclen=0)
#aplico el modelo a los datos nuevos
prediccion  <- predict( object= modelo,
newdata= dapply,
type = "prob")
#prediccion es una matriz con TRES columnas, llamadas "BAJA+1", "BAJA+2"  y "CONTINUA"
#cada columna es el vector de probabilidades
#agrego a dapply una columna nueva que es la probabilidad de BAJA+2
dapply[ , prob_baja2 := prediccion[, "BAJA+2"] ]
#solo le envio estimulo a los registros con probabilidad de BAJA+2 mayor  a  1/40
dapply[ , Predicted := as.numeric( prob_baja2 > 1/40 ) ]
#genero el archivo para Kaggle
#primero creo la carpeta donde va el experimento
dir.create( "./exp/" )
dir.create( "./exp/KA2001" )
fwrite( dapply[ , list(numero_de_cliente, Predicted) ], #solo los campos para Kaggle
file= "./exp/KA2001/K101_00150.csv",
sep=  "," )
gc()
gc()
#Arbol elemental con libreria  rpart
#Debe tener instaladas las librerias  data.table  ,  rpart  y  rpart.plot
#cargo las librerias que necesito
require("data.table")
require("rpart")
require("rpart.plot")
#Aqui se debe poner la carpeta de la materia de SU computadora local
setwd("C:/Users/varon/OneDrive/Escritorio/Data_mining")  #Establezco el Working Directory
#cargo el dataset
dataset  <- fread("./datasets/dataset_pequeno.csv")
dtrain  <- dataset[ foto_mes==202107 ]  #defino donde voy a entrenar
dapply  <- dataset[ foto_mes==202109 ]  #defino donde voy a aplicar el modelo
#genero el modelo,  aqui se construye el arbol
modelo  <- rpart(formula=   "clase_ternaria ~ .",  #quiero predecir clase_ternaria a partir de el resto de las variables
data=      dtrain,  #los datos donde voy a entrenar
xval=      0,
cp=       -0.5,     #esto significa no limitar la complejidad de los splits
minsplit=  200,     #minima cantidad de registros para que se haga el split
minbucket= 8,     #tamaño minimo de una hoja
maxdepth=  6 )    #profundidad maxima del arbol
#grafico el arbol
prp(modelo, extra=101, digits=-5, branch=1, type=4, varlen=0, faclen=0)
#aplico el modelo a los datos nuevos
prediccion  <- predict( object= modelo,
newdata= dapply,
type = "prob")
#prediccion es una matriz con TRES columnas, llamadas "BAJA+1", "BAJA+2"  y "CONTINUA"
#cada columna es el vector de probabilidades
#agrego a dapply una columna nueva que es la probabilidad de BAJA+2
dapply[ , prob_baja2 := prediccion[, "BAJA+2"] ]
#solo le envio estimulo a los registros con probabilidad de BAJA+2 mayor  a  1/40
dapply[ , Predicted := as.numeric( prob_baja2 > 1/40 ) ]
#genero el archivo para Kaggle
#primero creo la carpeta donde va el experimento
dir.create( "./exp/" )
dir.create( "./exp/KA2001" )
fwrite( dapply[ , list(numero_de_cliente, Predicted) ], #solo los campos para Kaggle
file= "./exp/KA2001/K101_00150.csv",
sep=  "," )
#Arbol elemental con libreria  rpart
#Debe tener instaladas las librerias  data.table  ,  rpart  y  rpart.plot
#cargo las librerias que necesito
require("data.table")
require("rpart")
require("rpart.plot")
#Aqui se debe poner la carpeta de la materia de SU computadora local
setwd("C:/Users/varon/OneDrive/Escritorio/Data_mining")  #Establezco el Working Directory
#cargo el dataset
dataset  <- fread("./datasets/dataset_pequeno.csv")
dtrain  <- dataset[ foto_mes==202107 ]  #defino donde voy a entrenar
dapply  <- dataset[ foto_mes==202109 ]  #defino donde voy a aplicar el modelo
#genero el modelo,  aqui se construye el arbol
modelo  <- rpart(formula=   "clase_ternaria ~ .",  #quiero predecir clase_ternaria a partir de el resto de las variables
data=      dtrain,  #los datos donde voy a entrenar
xval=      0,
cp=       -0.5,     #esto significa no limitar la complejidad de los splits
minsplit=  200,     #minima cantidad de registros para que se haga el split
minbucket= 8,     #tamaño minimo de una hoja
maxdepth=  6 )    #profundidad maxima del arbol
#grafico el arbol
prp(modelo, extra=101, digits=-5, branch=1, type=4, varlen=0, faclen=0)
#aplico el modelo a los datos nuevos
prediccion  <- predict( object= modelo,
newdata= dapply,
type = "prob")
#prediccion es una matriz con TRES columnas, llamadas "BAJA+1", "BAJA+2"  y "CONTINUA"
#cada columna es el vector de probabilidades
#agrego a dapply una columna nueva que es la probabilidad de BAJA+2
dapply[ , prob_baja2 := prediccion[, "BAJA+2"] ]
#solo le envio estimulo a los registros con probabilidad de BAJA+2 mayor  a  1/40
dapply[ , Predicted := as.numeric( prob_baja2 > 1/40 ) ]
#genero el archivo para Kaggle
#primero creo la carpeta donde va el experimento
dir.create( "./exp/" )
dir.create( "./exp/KA2001" )
fwrite( dapply[ , list(numero_de_cliente, Predicted) ], #solo los campos para Kaggle
file= "./exp/KA2001/K101_001100.csv",
sep=  "," )
#Arbol elemental con libreria  rpart
#Debe tener instaladas las librerias  data.table  ,  rpart  y  rpart.plot
#cargo las librerias que necesito
require("data.table")
require("rpart")
require("rpart.plot")
#Aqui se debe poner la carpeta de la materia de SU computadora local
setwd("C:/Users/varon/OneDrive/Escritorio/Data_mining")  #Establezco el Working Directory
#cargo el dataset
dataset  <- fread("./datasets/dataset_pequeno.csv")
dtrain  <- dataset[ foto_mes==202107 ]  #defino donde voy a entrenar
dapply  <- dataset[ foto_mes==202109 ]  #defino donde voy a aplicar el modelo
#genero el modelo,  aqui se construye el arbol
modelo  <- rpart(formula=   "clase_ternaria ~ .",  #quiero predecir clase_ternaria a partir de el resto de las variables
data=      dtrain,  #los datos donde voy a entrenar
xval=      0,
cp=       -1,     #esto significa no limitar la complejidad de los splits
minsplit=  400,     #minima cantidad de registros para que se haga el split
minbucket= 200,     #tamaño minimo de una hoja
maxdepth=  8 )    #profundidad maxima del arbol
#grafico el arbol
prp(modelo, extra=101, digits=-5, branch=1, type=4, varlen=0, faclen=0)
#aplico el modelo a los datos nuevos
prediccion  <- predict( object= modelo,
newdata= dapply,
type = "prob")
#prediccion es una matriz con TRES columnas, llamadas "BAJA+1", "BAJA+2"  y "CONTINUA"
#cada columna es el vector de probabilidades
#agrego a dapply una columna nueva que es la probabilidad de BAJA+2
dapply[ , prob_baja2 := prediccion[, "BAJA+2"] ]
#solo le envio estimulo a los registros con probabilidad de BAJA+2 mayor  a  1/40
dapply[ , Predicted := as.numeric( prob_baja2 > 1/40 ) ]
#genero el archivo para Kaggle
#primero creo la carpeta donde va el experimento
dir.create( "./exp/" )
dir.create( "./exp/KA2001" )
fwrite( dapply[ , list(numero_de_cliente, Predicted) ], #solo los campos para Kaggle
file= "./exp/KA2001/K101_00111.csv",
sep=  "," )
#Ensemble de arboles de decision
#utilizando el naif metodo de Arboles Azarosos
#entreno cada arbol utilizando un subconjunto distinto de atributos del dataset
#Para detener el script y mostrar el stack ante un error
options(error = function() {
traceback(20);
options(error = NULL);
stop("\nAbortando luego de un error fatal en el script.\n")
})
#limpio la memoria
rm( list=ls() )  #Borro todos los objetos
gc()   #Garbage Collection
require("data.table")
require("rpart")
# INICIO parametros experimento
PARAM <- list()
PARAM$experimento  <- 32130
PARAM$semilla  <- 999959      #Establezco la semilla aleatoria, cambiar por SU primer semilla
#parameetros rpart
PARAM$rpart_param   <- list( "cp"=          -1,
"minsplit"=  400,
"minbucket"=  200,
"maxdepth"=   8 )
#parametros  arbol
PARAM$feature_fraction  <- 0.5  #entreno cada arbol con solo 50% de las variables variables
PARAM$num_trees_max  <- 500 #voy a generar 500 arboles, a mas arboles mas tiempo de proceso y MEJOR MODELO, pero ganancias marginales
# FIN parametros experimento
#------------------------------------------------------------------------------
#------------------------------------------------------------------------------
#Aqui comienza el programa
setwd("C:/Users/varon/OneDrive/Escritorio/Data_mining")  #Establezco el Working Directory
#cargo los datos
dataset  <- fread("./datasets/dataset_pequeno.csv")
#creo la carpeta donde va el experimento
dir.create( "./exp/", showWarnings = FALSE  )
carpeta_experimento  <-  paste0( "./exp/KA", PARAM$experimento, "/")
dir.create( paste0( "./exp/KA", PARAM$experimento, "/"),
showWarnings = FALSE )
setwd( carpeta_experimento )
#que tamanos de ensemble grabo a disco, pero siempre debo generar los 500
grabar  <- c( 1, 5, 10, 50, 100, 200, 500)
#defino los dataset de entrenamiento y aplicacion
dtrain  <- dataset[ foto_mes==202107 ]
dapply  <- dataset[ foto_mes==202109 ]
#aqui se va acumulando la probabilidad del ensemble
dapply[ , prob_acumulada := 0 ]
#Establezco cuales son los campos que puedo usar para la prediccion
#el copy() es por la Lazy Evaluation
campos_buenos  <- copy( setdiff(  colnames(dtrain) ,  c("clase_ternaria") ) )
#Genero las salidas
set.seed(PARAM$semilla) #Establezco la semilla aleatoria
for( arbolito in  1:PARAM$num_trees_max )
{
qty_campos_a_utilizar  <- as.integer( length(campos_buenos)* PARAM$feature_fraction )
campos_random  <- sample( campos_buenos, qty_campos_a_utilizar )
#paso de un vector a un string con los elementos separados por un signo de "+"
#este hace falta para la formula
campos_random  <- paste( campos_random, collapse=" + ")
#armo la formula para rpart
formulita  <- paste0( "clase_ternaria ~ ", campos_random )
#genero el arbol de decision
modelo  <- rpart( formulita,
data= dtrain,
xval= 0,
control= PARAM$rpart_param )
#aplico el modelo a los datos que no tienen clase
prediccion  <- predict( modelo, dapply , type = "prob")
dapply[  ,  prob_acumulada :=  prob_acumulada + prediccion[ , "BAJA+2"] ]
if( arbolito %in%  grabar )
{
#Genero la entrega para Kaggle
umbral_corte  <-  (1/40) *  arbolito
entrega  <- as.data.table( list( "numero_de_cliente"= dapply[  , numero_de_cliente],
"Predicted"= as.numeric(dapply[  , prob_acumulada] > umbral_corte) ) ) #genero la salida
nom_arch  <- paste0('KA', PARAM$experimento, "_",
sprintf( '%.3d', arbolito ), #para que tenga ceros adelante
'.csv' )
fwrite( entrega,
file= nom_arch,
sep= "," )
cat( arbolito, " " )
}
cat( arbolito, "\n" )  #imprimo cada pasada para no angustiarme
}
#Ensemble de arboles de decision
#utilizando el naif metodo de Arboles Azarosos
#entreno cada arbol utilizando un subconjunto distinto de atributos del dataset
#Para detener el script y mostrar el stack ante un error
options(error = function() {
traceback(20);
options(error = NULL);
stop("\nAbortando luego de un error fatal en el script.\n")
})
#limpio la memoria
rm( list=ls() )  #Borro todos los objetos
gc()   #Garbage Collection
require("data.table")
require("rpart")
# INICIO parametros experimento
PARAM <- list()
PARAM$experimento  <- 32130
PARAM$semilla  <- 999959      #Establezco la semilla aleatoria, cambiar por SU primer semilla
#parameetros rpart
PARAM$rpart_param   <- list( "cp"=          -1,
"minsplit"=  400,
"minbucket"=  200,
"maxdepth"=   8 )
#parametros  arbol
PARAM$feature_fraction  <- 0.5  #entreno cada arbol con solo 50% de las variables variables
PARAM$num_trees_max  <- 500 #voy a generar 500 arboles, a mas arboles mas tiempo de proceso y MEJOR MODELO, pero ganancias marginales
# FIN parametros experimento
#------------------------------------------------------------------------------
#------------------------------------------------------------------------------
#Aqui comienza el programa
setwd("C:/Users/varon/OneDrive/Escritorio/Data_mining")  #Establezco el Working Directory
#cargo los datos
dataset  <- fread("./datasets/dataset_pequeno.csv")
#creo la carpeta donde va el experimento
dir.create( "./exp/", showWarnings = FALSE  )
carpeta_experimento  <-  paste0( "./exp/KA", PARAM$experimento, "/")
dir.create( paste0( "./exp/KA", PARAM$experimento, "/"),
showWarnings = FALSE )
setwd( carpeta_experimento )
#que tamanos de ensemble grabo a disco, pero siempre debo generar los 500
grabar  <- c( 1, 5, 10, 50, 100, 200, 500)
#defino los dataset de entrenamiento y aplicacion
dtrain  <- dataset[ foto_mes==202107 ]
dapply  <- dataset[ foto_mes==202109 ]
#aqui se va acumulando la probabilidad del ensemble
dapply[ , prob_acumulada := 0 ]
#Establezco cuales son los campos que puedo usar para la prediccion
#el copy() es por la Lazy Evaluation
campos_buenos  <- copy( setdiff(  colnames(dtrain) ,  c("clase_ternaria") ) )
#Genero las salidas
set.seed(PARAM$semilla) #Establezco la semilla aleatoria
for( arbolito in  1:PARAM$num_trees_max )
{
qty_campos_a_utilizar  <- as.integer( length(campos_buenos)* PARAM$feature_fraction )
campos_random  <- sample( campos_buenos, qty_campos_a_utilizar )
#paso de un vector a un string con los elementos separados por un signo de "+"
#este hace falta para la formula
campos_random  <- paste( campos_random, collapse=" + ")
#armo la formula para rpart
formulita  <- paste0( "clase_ternaria ~ ", campos_random )
#genero el arbol de decision
modelo  <- rpart( formulita,
data= dtrain,
xval= 0,
control= PARAM$rpart_param )
#aplico el modelo a los datos que no tienen clase
prediccion  <- predict( modelo, dapply , type = "prob")
dapply[  ,  prob_acumulada :=  prob_acumulada + prediccion[ , "BAJA+2"] ]
if( arbolito %in%  grabar )
{
#Genero la entrega para Kaggle
umbral_corte  <-  (1/40) *  arbolito
entrega  <- as.data.table( list( "numero_de_cliente"= dapply[  , numero_de_cliente],
"Predicted"= as.numeric(dapply[  , prob_acumulada] > umbral_corte) ) ) #genero la salida
nom_arch  <- paste0('KA', PARAM$experimento, "_",
sprintf( '%.3d', arbolito ), #para que tenga ceros adelante
'.csv' )
fwrite( entrega,
file= nom_arch,
sep= "," )
cat( arbolito, " " )
}
cat( arbolito, "\n" )  #imprimo cada pasada para no angustiarme
}
