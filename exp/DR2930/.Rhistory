AIC <- c(summary(ses_opt)$ARIMA$aic, autoARIMA$aic)
RMSE <- c(accuracy(predARIMA, nottem_prueba)[2,2],
accuracy(pred_autoARIMA, nottem_prueba)[2,2])
MAPE <- c(
accuracy(predARIMA, nottem_prueba)[2,5],
accuracy(pred_autoARIMA, nottem_prueba)[2,5])
data.frame(Modelo = Modelo, AIC = AIC, RMSE = RMSE, MAPE = MAPE)
Modelo <- c("ARIMA(25,1,37)", "AutoARIMA")
AIC <- c(summary(ses_opt)$ARIMA$aic, $autoARIMA$aic)
Modelo <- c("ARIMA(25,1,37)", "AutoARIMA")
AIC <- c(summary(ses_opt)$ARIMA$aic, autoARIMA$aic)
RMSE <- c(accuracy(predARIMA, nottem_prueba)[2,2],
accuracy(pred_autoARIMA, nottem_prueba)[2,2])
MAPE <- c(
accuracy(predARIMA, nottem_prueba)[2,5],
accuracy(pred_autoARIMA, nottem_prueba)[2,5])
data.frame(Modelo = Modelo, AIC = AIC, RMSE = RMSE, MAPE = MAPE)
Modelo <- c("ARIMA(25,1,37)", "AutoARIMA")
AIC <- c(summary(ses_opt)$ARIMA$aic, autoARIMA$aic)
RMSE <- c(accuracy(predARIMA, nottem_prueba)[2,2],
accuracy(pred_autoARIMA, nottem_prueba)[2,2])
MAPE <- c(
accuracy(predARIMA, nottem_prueba)[2,5],
accuracy(pred_autoARIMA, nottem_prueba)[2,5])
data.frame(Modelo = Modelo, AIC = AIC, RMSE = RMSE, MAPE = MAPE)
install.packages(c('repr', 'IRdisplay', 'evaluate', 'crayon') )
install.packages(c("repr", "IRdisplay", "evaluate", "crayon"))
install.packages(c('pbdZMQ', 'devtools', 'uuid', 'digest') )
install.packages('IRkernel')
library( "IRkernel" ) IRkernel::installspec()
library( "IRkernel" )
library( "IRkernel" ) IRkernel::installspec()
IRkernel::installspec()
library( "IRkernel" ) IRkernel::installspec()
library( "IRkernel" )
IRkernel::installspec()
IRkernel::installspec()
install.packages(c('repr', 'IRdisplay', 'evaluate', 'crayon') ) install.packages(c('pbdZMQ', 'devtools', 'uuid', 'digest') ) install.packages('IRkernel')
install.packages(c('repr', 'IRdisplay', 'evaluate', 'crayon') )
install.packages(c('pbdZMQ', 'devtools', 'uuid', 'digest') )
install.packages('IRkernel')
library( "IRkernel" )
IRkernel::installspec()
quit()
pacman::p_load(rvest,dplyr,xml2,tidyverse,RCurl,XML,readxl,splitstackshape,writexl)
# Function to extract text from long articles
extract_body_text <- function(text) {
if (nchar(text, type = "chars", allowNA = FALSE, keepNA = NA) > 32000) {
sentences <- unlist(strsplit(text, "ред", perl = TRUE))
num_sentences <- length(sentences)
if (num_sentences < 2) {
sentences <- unlist(strsplit(text, ".", perl = TRUE))
num_sentences <- length(sentences)
}
body_text <- ""
for (n in 1:num_sentences) {
txt_sent <- sentences[n]
body_text <- txt_sent
body_text <- str_c(body_text, collapse = "")
}
} else {
body_text <- text
}
return(body_text)
}
list_of_files <- list.files(path = "C:/Users/varon/OneDrive/Escritorio/investigacion/bases de datos/Arabia_Saudita", recursive = TRUE,
pattern = "\\.csv$",
full.names = FALSE)
links <- character()
for (i in 1:length(list_of_files)){
txt1 <- paste("ua_temp <-read.csv('C:/Users/varon/OneDrive/Escritorio/investigacion/bases de datos/Arabia_Saudita/", list_of_files[i],"')",sep="")
eval(parse(text=txt1))
ua_temp <- ua_temp[,1]
if (i==1){
links <- ua_temp
} else {
links <- c(links,ua_temp)
}
print(paste("File", i, "processed."))
}
links <- links[grepl("https://www.mofa.gov.sa/", links, fixed = TRUE)]
links <- links[!grepl("https://www.mofa.gov.sa/aboutKingDom/", links, fixed = TRUE)]
links <- links[!grepl("https://www.mofa.gov.sa/aboutMinistry", links, fixed = TRUE)]
links <- links[!grepl("https://www.mofa.gov.sa/ar/ministry/Pages/", links, fixed = TRUE)]
links <- sort(links)
links<-unique(links)
# Base mistakes
Arabia_Saudita <- data.frame(url = links, title = NA, date = NA, text = NA, stringsAsFactors = FALSE)
Arabia_Saudita.Mistakes <- data.frame(url = character(),
stringsAsFactors = FALSE)
# Unique
Arabia_Saudita<- unique(Arabia_Saudita)
for (i in seq_len(nrow(Arabia_Saudita))) {
url <- Arabia_Saudita$url[i]
tryCatch({
html <- read_html(url)
title <- html_text(html_node(html, "#pageTitle"))
date <- html_text(html_node(html, ".date-line")) #
#date <- date[endsWith(date,'. m.') | endsWith(date,'.m.')]
# date<- gsub("-.*","",date)
text <- html_text(html_node(html, ".page-content"))
date <- str_squish(date)
title <- str_squish(title)
text <- str_squish(text)
body_text <- extract_body_text(text)
Arabia_Saudita$title[i] <- title
Arabia_Saudita$date[i] <- date
Arabia_Saudita$text[i] <- body_text
cat(paste0("Link ", i, " of ", nrow(Arabia_Saudita), " processed successfully.\n"))
}, error = function(e) {
cat(paste0("Link ", i, " of ", nrow(Arabia_Saudita), " failed with error: ", e$message, "\n"))
Arabia_Saudita.Mistakes <<- rbind(Arabia_Saudita.Mistakes, data.frame(url = url), stringsAsFactors = FALSE)
})
}
View(Arabia_Saudita)
pacman::p_load(rvest,dplyr,xml2,tidyverse,RCurl,XML,readxl,splitstackshape,writexl)
# Function to extract text from long articles
extract_body_text <- function(text) {
if (nchar(text, type = "chars", allowNA = FALSE, keepNA = NA) > 32000) {
sentences <- unlist(strsplit(text, "ред", perl = TRUE))
num_sentences <- length(sentences)
if (num_sentences < 2) {
sentences <- unlist(strsplit(text, ".", perl = TRUE))
num_sentences <- length(sentences)
}
body_text <- ""
for (n in 1:num_sentences) {
txt_sent <- sentences[n]
body_text <- txt_sent
body_text <- str_c(body_text, collapse = "")
}
} else {
body_text <- text
}
return(body_text)
}
list_of_files <- list.files(path = "C:/Users/varon/OneDrive/Escritorio/investigacion/bases de datos/Arabia_Saudita", recursive = TRUE,
pattern = "\\.csv$",
full.names = FALSE)
links <- character()
for (i in 1:length(list_of_files)){
txt1 <- paste("ua_temp <-read.csv('C:/Users/varon/OneDrive/Escritorio/investigacion/bases de datos/Arabia_Saudita/", list_of_files[i],"')",sep="")
eval(parse(text=txt1))
ua_temp <- ua_temp[,1]
if (i==1){
links <- ua_temp
} else {
links <- c(links,ua_temp)
}
print(paste("File", i, "processed."))
}
links <- links[grepl("https://www.mofa.gov.sa/", links, fixed = TRUE)]
links <- links[!grepl("https://www.mofa.gov.sa/aboutKingDom/", links, fixed = TRUE)]
links <- links[!grepl("https://www.mofa.gov.sa/aboutMinistry", links, fixed = TRUE)]
links <- links[!grepl("https://www.mofa.gov.sa/ar/ministry/Pages/", links, fixed = TRUE)]
links <- sort(links)
links<-unique(links)
# Base mistakes
Arabia_Saudita <- data.frame(url = links, title = NA, date = NA, text = NA, stringsAsFactors = FALSE)
Arabia_Saudita.Mistakes <- data.frame(url = character(),
stringsAsFactors = FALSE)
# Unique
Arabia_Saudita<- unique(Arabia_Saudita)
for (i in seq_len(nrow(Arabia_Saudita))) {
url <- Arabia_Saudita$url[i]
tryCatch({
html <- read_html(url)
title <- html_text(html_node(html, "#DeltaPlaceHolderPageTitleInTitleArea"))
date <- html_text(html_node(html, ".date-line")) #
#date <- date[endsWith(date,'. m.') | endsWith(date,'.m.')]
# date<- gsub("-.*","",date)
text <- html_text(html_node(html, ".page-content"))
date <- str_squish(date)
title <- str_squish(title)
text <- str_squish(text)
body_text <- extract_body_text(text)
Arabia_Saudita$title[i] <- title
Arabia_Saudita$date[i] <- date
Arabia_Saudita$text[i] <- body_text
cat(paste0("Link ", i, " of ", nrow(Arabia_Saudita), " processed successfully.\n"))
}, error = function(e) {
cat(paste0("Link ", i, " of ", nrow(Arabia_Saudita), " failed with error: ", e$message, "\n"))
Arabia_Saudita.Mistakes <<- rbind(Arabia_Saudita.Mistakes, data.frame(url = url), stringsAsFactors = FALSE)
})
}
View(Arabia_Saudita)
pacman::p_load(rvest,dplyr,xml2,tidyverse,RCurl,XML,readxl,splitstackshape,writexl)
# Function to extract text from long articles
extract_body_text <- function(text) {
if (nchar(text, type = "chars", allowNA = FALSE, keepNA = NA) > 32000) {
sentences <- unlist(strsplit(text, "ред", perl = TRUE))
num_sentences <- length(sentences)
if (num_sentences < 2) {
sentences <- unlist(strsplit(text, ".", perl = TRUE))
num_sentences <- length(sentences)
}
body_text <- ""
for (n in 1:num_sentences) {
txt_sent <- sentences[n]
body_text <- txt_sent
body_text <- str_c(body_text, collapse = "")
}
} else {
body_text <- text
}
return(body_text)
}
list_of_files <- list.files(path = "C:/Users/varon/OneDrive/Escritorio/investigacion/bases de datos/Arabia_Saudita", recursive = TRUE,
pattern = "\\.csv$",
full.names = FALSE)
links <- character()
for (i in 1:length(list_of_files)){
txt1 <- paste("ua_temp <-read.csv('C:/Users/varon/OneDrive/Escritorio/investigacion/bases de datos/Arabia_Saudita/", list_of_files[i],"')",sep="")
eval(parse(text=txt1))
ua_temp <- ua_temp[,1]
if (i==1){
links <- ua_temp
} else {
links <- c(links,ua_temp)
}
print(paste("File", i, "processed."))
}
links <- links[grepl("https://www.mofa.gov.sa/", links, fixed = TRUE)]
links <- links[!grepl("https://www.mofa.gov.sa/aboutKingDom/", links, fixed = TRUE)]
links <- links[!grepl("https://www.mofa.gov.sa/aboutMinistry", links, fixed = TRUE)]
links <- links[!grepl("https://www.mofa.gov.sa/ar/ministry/Pages/", links, fixed = TRUE)]
links <- links[grepl("https://www.mofa.gov.sa/en/ministry/news/", links, fixed = TRUE)]
links <- sort(links)
links<-unique(links)
# Base mistakes
Arabia_Saudita <- data.frame(url = links, title = NA, date = NA, text = NA, stringsAsFactors = FALSE)
Arabia_Saudita.Mistakes <- data.frame(url = character(),
stringsAsFactors = FALSE)
# Unique
Arabia_Saudita<- unique(Arabia_Saudita)
for (i in seq_len(nrow(Arabia_Saudita))) {
url <- Arabia_Saudita$url[i]
tryCatch({
html <- read_html(url)
title <- html_text(html_node(html, "#DeltaPlaceHolderPageTitleInTitleArea"))
date <- html_text(html_node(html, ".date-line")) #
#date <- date[endsWith(date,'. m.') | endsWith(date,'.m.')]
# date<- gsub("-.*","",date)
text <- html_text(html_node(html, ".page-content"))
date <- str_squish(date)
title <- str_squish(title)
text <- str_squish(text)
body_text <- extract_body_text(text)
Arabia_Saudita$title[i] <- title
Arabia_Saudita$date[i] <- date
Arabia_Saudita$text[i] <- body_text
cat(paste0("Link ", i, " of ", nrow(Arabia_Saudita), " processed successfully.\n"))
}, error = function(e) {
cat(paste0("Link ", i, " of ", nrow(Arabia_Saudita), " failed with error: ", e$message, "\n"))
Arabia_Saudita.Mistakes <<- rbind(Arabia_Saudita.Mistakes, data.frame(url = url), stringsAsFactors = FALSE)
})
}
View(Arabia_Saudita)
#Ensemble de arboles de decision
#utilizando el naif metodo de Arboles Azarosos
#entreno cada arbol utilizando un subconjunto distinto de atributos del dataset
#Para detener el script y mostrar el stack ante un error
options(error = function() {
traceback(20);
options(error = NULL);
stop("\nAbortando luego de un error fatal en el script.\n")
})
#limpio la memoria
rm( list=ls() )  #Borro todos los objetos
gc()   #Garbage Collection
require("data.table")
require("rpart")
# INICIO parametros experimento
PARAM <- list()
PARAM$experimento  <- 3219999
PARAM$semilla  <- 999959      #Establezco la semilla aleatoria, cambiar por SU primer semilla
#parameetros rpart
PARAM$rpart_param   <- list( "cp"=          -1,
"minsplit"=  500,
"minbucket"=  100,
"maxdepth"=   14 )
#parametros  arbol
PARAM$feature_fraction  <- 0.5  #entreno cada arbol con solo 50% de las variables variables
PARAM$num_trees_max  <- 500 #voy a generar 500 arboles, a mas arboles mas tiempo de proceso y MEJOR MODELO, pero ganancias marginales
# FIN parametros experimento
#------------------------------------------------------------------------------
#------------------------------------------------------------------------------
#Aqui comienza el programa
setwd("C:/Users/varon/OneDrive/Escritorio/Data_mining")  #Establezco el Working Directory
#cargo los datos
dataset  <- fread("./datasets/dataset_pequeno.csv")
#creo la carpeta donde va el experimento
dir.create( "./exp/", showWarnings = FALSE  )
carpeta_experimento  <-  paste0( "./exp/KA", PARAM$experimento, "/")
dir.create( paste0( "./exp/KA", PARAM$experimento, "/"),
showWarnings = FALSE )
setwd( carpeta_experimento )
#que tamanos de ensemble grabo a disco, pero siempre debo generar los 500
grabar  <- c( 1, 5, 10, 50, 100, 200, 500)
#defino los dataset de entrenamiento y aplicacion
dtrain  <- dataset[ foto_mes==202107 ]
dapply  <- dataset[ foto_mes==202109 ]
#aqui se va acumulando la probabilidad del ensemble
dapply[ , prob_acumulada := 0 ]
#Establezco cuales son los campos que puedo usar para la prediccion
#el copy() es por la Lazy Evaluation
campos_buenos  <- copy( setdiff(  colnames(dtrain) ,  c("clase_ternaria") ) )
#Genero las salidas
set.seed(PARAM$semilla) #Establezco la semilla aleatoria
for( arbolito in  1:PARAM$num_trees_max )
{
qty_campos_a_utilizar  <- as.integer( length(campos_buenos)* PARAM$feature_fraction )
campos_random  <- sample( campos_buenos, qty_campos_a_utilizar )
#paso de un vector a un string con los elementos separados por un signo de "+"
#este hace falta para la formula
campos_random  <- paste( campos_random, collapse=" + ")
#armo la formula para rpart
formulita  <- paste0( "clase_ternaria ~ ", campos_random )
#genero el arbol de decision
modelo  <- rpart( formulita,
data= dtrain,
xval= 0,
control= PARAM$rpart_param )
#aplico el modelo a los datos que no tienen clase
prediccion  <- predict( modelo, dapply , type = "prob")
dapply[  ,  prob_acumulada :=  prob_acumulada + prediccion[ , "BAJA+2"] ]
if( arbolito %in%  grabar )
{
#Genero la entrega para Kaggle
umbral_corte  <-  (1/40) *  arbolito
entrega  <- as.data.table( list( "numero_de_cliente"= dapply[  , numero_de_cliente],
"Predicted"= as.numeric(dapply[  , prob_acumulada] > umbral_corte) ) ) #genero la salida
nom_arch  <- paste0('KA', PARAM$experimento, "_",
sprintf( '%.3d', arbolito ), #para que tenga ceros adelante
'.csv' )
fwrite( entrega,
file= nom_arch,
sep= "," )
cat( arbolito, " " )
}
cat( arbolito, "\n" )  #imprimo cada pasada para no angustiarme
}
# Script para encontrar Visuamente  el data drifting
# focalizado solo en los campos de un buen arbol de deicision
#limpio la memoria
rm( list=ls() )  #remove all objects
gc()             #garbage collection
require("data.table")
require("rpart")
#------------------------------------------------------------------------------
graficar_campo  <- function( campo )
{
#quito de grafico las colas del 5% de las densidades
qA  <- quantile(  dataset[ foto_mes==202107 , get(campo) ] , prob= c(0.05, 0.95), na.rm=TRUE )
qB  <- quantile(  dataset[ foto_mes==202109 , get(campo) ] , prob= c(0.05, 0.95), na.rm=TRUE )
xxmin  <- pmin( qA[[1]], qB[[1]] )
xxmax  <- pmax( qA[[2]], qB[[2]] )
densidad_A  <- density( dataset[ foto_mes==202107, get(campo) ],
kernel="gaussian", na.rm=TRUE )
densidad_B  <- density( dataset[ foto_mes==202109, get(campo) ],
kernel="gaussian", na.rm=TRUE )
plot( densidad_A,
col="blue",
xlim= c( xxmin, xxmax ),
ylim= c( 0, pmax( max(densidad_A$y), max(densidad_B$y) ) ),
main= campo
)
lines(densidad_B, col="red", lty=2)
legend(  "topright",
legend=c("202107", "202109"),
col=c("blue", "red"), lty=c(1,2))
}
#------------------------------------------------------------------------------
#Aqui comienza el programa
setwd("C:/Users/varon/OneDrive/Escritorio/Data_mining/dm2023a")   #Establezco el Working Directory
#cargo el dataset donde voy a entrenar
dataset  <- fread("./datasets/dataset_pequeno.csv")
dir.create( "./exp/",  showWarnings = FALSE )
dir.create( "./exp/DR2930/", showWarnings = FALSE )
setwd("./exp/DR2930/")
dataset  <- dataset[ foto_mes %in% c( 202107, 202109 ) ]
# Script para encontrar Visuamente  el data drifting
# focalizado solo en los campos de un buen arbol de deicision
#limpio la memoria
rm( list=ls() )  #remove all objects
gc()             #garbage collection
require("data.table")
require("rpart")
#------------------------------------------------------------------------------
graficar_campo  <- function( campo )
{
#quito de grafico las colas del 5% de las densidades
qA  <- quantile(  dataset[ foto_mes==202107 , get(campo) ] , prob= c(0.05, 0.95), na.rm=TRUE )
qB  <- quantile(  dataset[ foto_mes==202109 , get(campo) ] , prob= c(0.05, 0.95), na.rm=TRUE )
xxmin  <- pmin( qA[[1]], qB[[1]] )
xxmax  <- pmax( qA[[2]], qB[[2]] )
densidad_A  <- density( dataset[ foto_mes==202107, get(campo) ],
kernel="gaussian", na.rm=TRUE )
densidad_B  <- density( dataset[ foto_mes==202109, get(campo) ],
kernel="gaussian", na.rm=TRUE )
plot( densidad_A,
col="blue",
xlim= c( xxmin, xxmax ),
ylim= c( 0, pmax( max(densidad_A$y), max(densidad_B$y) ) ),
main= campo
)
lines(densidad_B, col="red", lty=2)
legend(  "topright",
legend=c("202107", "202109"),
col=c("blue", "red"), lty=c(1,2))
}
#------------------------------------------------------------------------------
#Aqui comienza el programa
setwd("C:/Users/varon/OneDrive/Escritorio/Data_mining/dm2023a")   #Establezco el Working Directory
#cargo el dataset donde voy a entrenar
dataset  <- fread("./datasets/dataset_pequeno.csv")
# Script para encontrar Visuamente  el data drifting
# focalizado solo en los campos de un buen arbol de deicision
#limpio la memoria
rm( list=ls() )  #remove all objects
gc()             #garbage collection
require("data.table")
require("rpart")
#------------------------------------------------------------------------------
graficar_campo  <- function( campo )
{
#quito de grafico las colas del 5% de las densidades
qA  <- quantile(  dataset[ foto_mes==202107 , get(campo) ] , prob= c(0.05, 0.95), na.rm=TRUE )
qB  <- quantile(  dataset[ foto_mes==202109 , get(campo) ] , prob= c(0.05, 0.95), na.rm=TRUE )
xxmin  <- pmin( qA[[1]], qB[[1]] )
xxmax  <- pmax( qA[[2]], qB[[2]] )
densidad_A  <- density( dataset[ foto_mes==202107, get(campo) ],
kernel="gaussian", na.rm=TRUE )
densidad_B  <- density( dataset[ foto_mes==202109, get(campo) ],
kernel="gaussian", na.rm=TRUE )
plot( densidad_A,
col="blue",
xlim= c( xxmin, xxmax ),
ylim= c( 0, pmax( max(densidad_A$y), max(densidad_B$y) ) ),
main= campo
)
lines(densidad_B, col="red", lty=2)
legend(  "topright",
legend=c("202107", "202109"),
col=c("blue", "red"), lty=c(1,2))
}
#------------------------------------------------------------------------------
#Aqui comienza el programa
setwd("C:/Users/varon/OneDrive/Escritorio/Data_mining/dm2023a/")   #Establezco el Working Directory
#cargo el dataset donde voy a entrenar
dataset  <- fread("./datasets/dataset_pequeno.csv")
# Script para encontrar Visuamente  el data drifting
# focalizado solo en los campos de un buen arbol de deicision
#limpio la memoria
rm( list=ls() )  #remove all objects
gc()             #garbage collection
require("data.table")
require("rpart")
#------------------------------------------------------------------------------
graficar_campo  <- function( campo )
{
#quito de grafico las colas del 5% de las densidades
qA  <- quantile(  dataset[ foto_mes==202107 , get(campo) ] , prob= c(0.05, 0.95), na.rm=TRUE )
qB  <- quantile(  dataset[ foto_mes==202109 , get(campo) ] , prob= c(0.05, 0.95), na.rm=TRUE )
xxmin  <- pmin( qA[[1]], qB[[1]] )
xxmax  <- pmax( qA[[2]], qB[[2]] )
densidad_A  <- density( dataset[ foto_mes==202107, get(campo) ],
kernel="gaussian", na.rm=TRUE )
densidad_B  <- density( dataset[ foto_mes==202109, get(campo) ],
kernel="gaussian", na.rm=TRUE )
plot( densidad_A,
col="blue",
xlim= c( xxmin, xxmax ),
ylim= c( 0, pmax( max(densidad_A$y), max(densidad_B$y) ) ),
main= campo
)
lines(densidad_B, col="red", lty=2)
legend(  "topright",
legend=c("202107", "202109"),
col=c("blue", "red"), lty=c(1,2))
}
#------------------------------------------------------------------------------
#Aqui comienza el programa
setwd("C:/Users/varon/OneDrive/Escritorio/Data_mining")   #Establezco el Working Directory
#cargo el dataset donde voy a entrenar
dataset  <- fread("./datasets/dataset_pequeno.csv")
dir.create( "./exp/",  showWarnings = FALSE )
dir.create( "./exp/DR2930/", showWarnings = FALSE )
setwd("./exp/DR2930/")
dataset  <- dataset[ foto_mes %in% c( 202107, 202109 ) ]
#creo la clase_binaria SI={ BAJA+1, BAJA+2 }    NO={ CONTINUA }
dataset[ foto_mes==202107,
clase_binaria :=  ifelse( clase_ternaria=="CONTINUA", "NO", "SI" ) ]
# Entreno el modelo
# utilizo los mejores hiperparametros encontrados en una Bayesian Optimizationcon 5-fold Cross Validation
modelo  <- rpart(formula=   "clase_binaria ~ . -clase_ternaria",
data=      dataset[ foto_mes==202107 ],  #los datos donde voy a entrenar
xval=          0,
cp=           -0.69,
minsplit=    870,
minbucket=     9,
maxdepth=      9)
campos_modelo  <- names( modelo$variable.importance )
campos_buenos  <- c( campos_modelo,  setdiff( colnames(dataset), campos_modelo ) )
campos_buenos  <-  setdiff(  campos_buenos,  c( "foto_mes","clase_ternaria","clase_binaria" ) )
pdf("densidades_07_09.pdf")
for( campo in  campos_buenos )
{
cat( campo, "  " )
graficar_campo( campo )
}
dev.off()
