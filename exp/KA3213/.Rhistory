for (i in 1:length(list_of_files)){
txt1 <- paste("ua_temp <-read.csv('C:/Users/varon/OneDrive/Escritorio/investigacion/bases de datos/Arabia_Saudita/", list_of_files[i],"')",sep="")
eval(parse(text=txt1))
ua_temp <- ua_temp[,1]
if (i==1){
links <- ua_temp
} else {
links <- c(links,ua_temp)
}
print(paste("File", i, "processed."))
}
links <- links[grepl("https://www.mofa.gov.sa/", links, fixed = TRUE)]
links <- links[!grepl("https://www.mofa.gov.sa/aboutKingDom/", links, fixed = TRUE)]
links <- sort(links)
links<-unique(links)
# Base mistakes
Arabia_Saudita <- data.frame(url = links, title = NA, date = NA, text = NA, stringsAsFactors = FALSE)
Arabia_Saudita.Mistakes <- data.frame(url = character(),
stringsAsFactors = FALSE)
# Unique
Arabia_Saudita<- unique(Arabia_Saudita)
for (i in seq_len(nrow(Arabia_Saudita))) {
url <- Arabia_Saudita$url[i]
tryCatch({
html <- read_html(url)
title <- html_text(html_node(html, "#pageTitle"))
date <- html_text(html_node(html, ".date-line")) #
#date <- date[endsWith(date,'. m.') | endsWith(date,'.m.')]
# date<- gsub("-.*","",date)
text <- html_text(html_node(html, ".page-content"))
date <- str_squish(date)
title <- str_squish(title)
text <- str_squish(text)
body_text <- extract_body_text(text)
Arabia_Saudita$title[i] <- title
Arabia_Saudita$date[i] <- date
Arabia_Saudita$text[i] <- body_text
cat(paste0("Link ", i, " of ", nrow(Arabia_Saudita), " processed successfully.\n"))
}, error = function(e) {
cat(paste0("Link ", i, " of ", nrow(Arabia_Saudita), " failed with error: ", e$message, "\n"))
Arabia_Saudita.Mistakes <<- rbind(Arabia_Saudita.Mistakes, data.frame(url = url), stringsAsFactors = FALSE)
})
}
View(Arabia_Saudita)
View(Arabia_Saudita)
pacman::p_load(rvest,dplyr,xml2,tidyverse,RCurl,XML,readxl,splitstackshape,writexl)
# Function to extract text from long articles
extract_body_text <- function(text) {
if (nchar(text, type = "chars", allowNA = FALSE, keepNA = NA) > 32000) {
sentences <- unlist(strsplit(text, "।", perl = TRUE))
num_sentences <- length(sentences)
if (num_sentences < 2) {
sentences <- unlist(strsplit(text, ".", perl = TRUE))
num_sentences <- length(sentences)
}
body_text <- ""
for (n in 1:num_sentences) {
txt_sent <- sentences[n]
body_text <- txt_sent
body_text <- str_c(body_text, collapse = "")
}
} else {
body_text <- text
}
return(body_text)
}
list_of_files <- list.files(path = "C:/Users/varon/OneDrive/Escritorio/investigacion/bases de datos/Arabia_Saudita", recursive = TRUE,
pattern = "\\.csv$",
full.names = FALSE)
links <- character()
for (i in 1:length(list_of_files)){
txt1 <- paste("ua_temp <-read.csv('C:/Users/varon/OneDrive/Escritorio/investigacion/bases de datos/Arabia_Saudita/", list_of_files[i],"')",sep="")
eval(parse(text=txt1))
ua_temp <- ua_temp[,1]
if (i==1){
links <- ua_temp
} else {
links <- c(links,ua_temp)
}
print(paste("File", i, "processed."))
}
links <- links[grepl("https://www.mofa.gov.sa/", links, fixed = TRUE)]
links <- links[!grepl("https://www.mofa.gov.sa/aboutKingDom/", links, fixed = TRUE)]
links <- links[!grepl("https://www.mofa.gov.sa/aboutMinistry", links, fixed = TRUE)]
links <- links[!grepl("https://www.mofa.gov.sa/ar/ministry/Pages/", links, fixed = TRUE)]
links <- sort(links)
links<-unique(links)
# Base mistakes
Arabia_Saudita <- data.frame(url = links, title = NA, date = NA, text = NA, stringsAsFactors = FALSE)
Arabia_Saudita.Mistakes <- data.frame(url = character(),
stringsAsFactors = FALSE)
# Unique
Arabia_Saudita<- unique(Arabia_Saudita)
for (i in seq_len(nrow(Arabia_Saudita))) {
url <- Arabia_Saudita$url[i]
tryCatch({
html <- read_html(url)
title <- html_text(html_node(html, "#pageTitle"))
date <- html_text(html_node(html, ".date-line")) #
#date <- date[endsWith(date,'. m.') | endsWith(date,'.m.')]
# date<- gsub("-.*","",date)
text <- html_text(html_node(html, ".page-content"))
date <- str_squish(date)
title <- str_squish(title)
text <- str_squish(text)
body_text <- extract_body_text(text)
Arabia_Saudita$title[i] <- title
Arabia_Saudita$date[i] <- date
Arabia_Saudita$text[i] <- body_text
cat(paste0("Link ", i, " of ", nrow(Arabia_Saudita), " processed successfully.\n"))
}, error = function(e) {
cat(paste0("Link ", i, " of ", nrow(Arabia_Saudita), " failed with error: ", e$message, "\n"))
Arabia_Saudita.Mistakes <<- rbind(Arabia_Saudita.Mistakes, data.frame(url = url), stringsAsFactors = FALSE)
})
}
View(Arabia_Saudita)
#esqueleto de grid search
#se espera que los alumnos completen lo que falta para recorrer TODOS cuatro los hiperparametros
#Para detener el script y mostrar el stack ante un error
options(error = function() {
traceback(20);
options(error = NULL);
stop("\nAbortando luego de un error fatal en el script.\n")
})
rm( list=ls() )  #Borro todos los objetos
gc()   #Garbage Collection
require("data.table")
require("rpart")
require("parallel")
ksemillas  <- c(999959, 999961, 999979, 999983, 999991) #reemplazar por las propias semillas
#------------------------------------------------------------------------------
#particionar agrega una columna llamada fold a un dataset que consiste en una particion estratificada segun agrupa
# particionar( data=dataset, division=c(70,30), agrupa=clase_ternaria, seed=semilla)   crea una particion 70, 30
particionar  <- function( data,  division, agrupa="",  campo="fold", start=1, seed=NA )
{
if( !is.na(seed) )   set.seed( seed )
bloque  <- unlist( mapply(  function(x,y) { rep( y, x )} ,   division,  seq( from=start, length.out=length(division) )  ) )
data[ , (campo) :=  sample( rep( bloque, ceiling(.N/length(bloque))) )[1:.N],
by= agrupa ]
}
#------------------------------------------------------------------------------
ArbolEstimarGanancia  <- function( semilla, param_basicos )
{
#particiono estratificadamente el dataset
particionar( dataset, division=c(7,3), agrupa="clase_ternaria", seed= semilla )
#genero el modelo
modelo  <- rpart("clase_ternaria ~ .",     #quiero predecir clase_ternaria a partir del resto
data= dataset[ fold==1],  #fold==1  es training,  el 70% de los datos
xval= 0,
control= param_basicos )  #aqui van los parametros del arbol
#aplico el modelo a los datos de testing
prediccion  <- predict( modelo,   #el modelo que genere recien
dataset[ fold==2],  #fold==2  es testing, el 30% de los datos
type= "prob") #type= "prob"  es que devuelva la probabilidad
#prediccion es una matriz con TRES columnas, llamadas "BAJA+1", "BAJA+2"  y "CONTINUA"
#cada columna es el vector de probabilidades
#calculo la ganancia en testing  qu es fold==2
ganancia_test  <- dataset[ fold==2,
sum( ifelse( prediccion[, "BAJA+2"]  >  0.025,
ifelse( clase_ternaria=="BAJA+2", 117000, -3000 ),
0 ) )]
#escalo la ganancia como si fuera todo el dataset
ganancia_test_normalizada  <-  ganancia_test / 0.3
return( ganancia_test_normalizada )
}
#------------------------------------------------------------------------------
ArbolesMontecarlo  <- function( semillas, param_basicos )
{
#la funcion mcmapply  llama a la funcion ArbolEstimarGanancia  tantas veces como valores tenga el vector  ksemillas
ganancias  <- mcmapply( ArbolEstimarGanancia,
semillas,   #paso el vector de semillas, que debe ser el primer parametro de la funcion ArbolEstimarGanancia
MoreArgs= list( param_basicos),  #aqui paso el segundo parametro
SIMPLIFY= FALSE,
mc.cores= 1 )  #se puede subir a 5 si posee Linux o Mac OS
ganancia_promedio  <- mean( unlist(ganancias) )
return( ganancia_promedio )
}
#------------------------------------------------------------------------------
#------------------------------------------------------------------------------
#Aqui se debe poner la carpeta de la computadora local
setwd("C:/Users/varon/OneDrive/Escritorio/Data_mining")  #Establezco el Working Directory
#cargo los datos
#cargo los datos
dataset  <- fread("./datasets/dataset_pequeno.csv")
#trabajo solo con los datos con clase, es decir 202107
dataset  <- dataset[ clase_ternaria!= "" ]
#genero el archivo para Kaggle
#creo la carpeta donde va el experimento
# HT  representa  Hiperparameter Tuning
dir.create( "./exp/",  showWarnings = FALSE )
dir.create( "./exp/HT2020/", showWarnings = FALSE )
archivo_salida  <- "./exp/HT2020/gridsearch.txt"
#Escribo los titulos al archivo donde van a quedar los resultados
#atencion que si ya existe el archivo, esta instruccion LO SOBREESCRIBE, y lo que estaba antes se pierde
#la forma que no suceda lo anterior es con append=TRUE
cat( file=archivo_salida,
sep= "",
"max_depth", "\t",
"min_split", "\t",
"ganancia_promedio", "\n")
#itero por los loops anidados para cada hiperparametro
#Aqui usted debera agregar loops !
curr_iter=0
for( vcp in  c( -1,-0.5, 0, 0.1 ) ){
for( vmax_depth  in  c( 4, 6, 8, 10, 12, 14 )  ) {
for( vmin_split  in  c( 1000, 800, 600, 400, 200, 100, 50, 20, 10 )  ){
for (vmin_bucket in  c(2, 4, 8, 16, 32, vmin_split/2, vmin_split/4 )){
total_iter <- length(c(-1, -0.5, 0, 0.1)) * length(c(4, 6, 8, 10, 12, 14)) * length(c(1000, 800, 600, 400, 200, 100, 50, 20, 10)) * length(c(2, 4, 8, 16, 32, vmin_split/2, vmin_split/4))
#notar como se agrega
param_basicos  <- list( "cp"=         vcp,       #complejidad minima
"minsplit"=  vmin_split,  #minima cantidad de registros en un nodo para hacer el split
"minbucket"=  vmin_bucket,#minima cantidad de registros en una hoja
"maxdepth"=  vmax_depth ) #profundidad máxima del arbol
#Un solo llamado, con la semilla 17
ganancia_promedio  <- ArbolesMontecarlo( ksemillas,  param_basicos )
#escribo los resultados al archivo de salida
cat(  file=archivo_salida,
append= TRUE,
sep= "",
vcp, "\t",
vmin_bucket, "\t",
vmax_depth, "\t",
vmin_split, "\t",
ganancia_promedio, "\n"  )
curr_iter <- curr_iter+1
cat("Iteration ", curr_iter, " of ", total_iter, "\n")
}
}
}
}
#esqueleto de grid search
#se espera que los alumnos completen lo que falta para recorrer TODOS cuatro los hiperparametros
#Para detener el script y mostrar el stack ante un error
options(error = function() {
traceback(20);
options(error = NULL);
stop("\nAbortando luego de un error fatal en el script.\n")
})
rm( list=ls() )  #Borro todos los objetos
gc()   #Garbage Collection
require("data.table")
require("rpart")
require("parallel")
ksemillas  <- c(999959, 999961, 999979, 999983, 999991) #reemplazar por las propias semillas
#------------------------------------------------------------------------------
#particionar agrega una columna llamada fold a un dataset que consiste en una particion estratificada segun agrupa
# particionar( data=dataset, division=c(70,30), agrupa=clase_ternaria, seed=semilla)   crea una particion 70, 30
particionar  <- function( data,  division, agrupa="",  campo="fold", start=1, seed=NA )
{
if( !is.na(seed) )   set.seed( seed )
bloque  <- unlist( mapply(  function(x,y) { rep( y, x )} ,   division,  seq( from=start, length.out=length(division) )  ) )
data[ , (campo) :=  sample( rep( bloque, ceiling(.N/length(bloque))) )[1:.N],
by= agrupa ]
}
#------------------------------------------------------------------------------
ArbolEstimarGanancia  <- function( semilla, param_basicos )
{
#particiono estratificadamente el dataset
particionar( dataset, division=c(7,3), agrupa="clase_ternaria", seed= semilla )
#genero el modelo
modelo  <- rpart("clase_ternaria ~ .",     #quiero predecir clase_ternaria a partir del resto
data= dataset[ fold==1],  #fold==1  es training,  el 70% de los datos
xval= 0,
control= param_basicos )  #aqui van los parametros del arbol
#aplico el modelo a los datos de testing
prediccion  <- predict( modelo,   #el modelo que genere recien
dataset[ fold==2],  #fold==2  es testing, el 30% de los datos
type= "prob") #type= "prob"  es que devuelva la probabilidad
#prediccion es una matriz con TRES columnas, llamadas "BAJA+1", "BAJA+2"  y "CONTINUA"
#cada columna es el vector de probabilidades
#calculo la ganancia en testing  qu es fold==2
ganancia_test  <- dataset[ fold==2,
sum( ifelse( prediccion[, "BAJA+2"]  >  0.025,
ifelse( clase_ternaria=="BAJA+2", 117000, -3000 ),
0 ) )]
#escalo la ganancia como si fuera todo el dataset
ganancia_test_normalizada  <-  ganancia_test / 0.3
return( ganancia_test_normalizada )
}
#------------------------------------------------------------------------------
ArbolesMontecarlo  <- function( semillas, param_basicos )
{
#la funcion mcmapply  llama a la funcion ArbolEstimarGanancia  tantas veces como valores tenga el vector  ksemillas
ganancias  <- mcmapply( ArbolEstimarGanancia,
semillas,   #paso el vector de semillas, que debe ser el primer parametro de la funcion ArbolEstimarGanancia
MoreArgs= list( param_basicos),  #aqui paso el segundo parametro
SIMPLIFY= FALSE,
mc.cores= 1 )  #se puede subir a 5 si posee Linux o Mac OS
ganancia_promedio  <- mean( unlist(ganancias) )
return( ganancia_promedio )
}
#------------------------------------------------------------------------------
#------------------------------------------------------------------------------
#Aqui se debe poner la carpeta de la computadora local
setwd("C:/Users/varon/OneDrive/Escritorio/Data_mining")  #Establezco el Working Directory
#cargo los datos
#cargo los datos
dataset  <- fread("./datasets/dataset_pequeno.csv")
#trabajo solo con los datos con clase, es decir 202107
dataset  <- dataset[ clase_ternaria!= "" ]
#genero el archivo para Kaggle
#creo la carpeta donde va el experimento
# HT  representa  Hiperparameter Tuning
dir.create( "./exp/",  showWarnings = FALSE )
dir.create( "./exp/HT2020/", showWarnings = FALSE )
archivo_salida  <- "./exp/HT2020/gridsearch.txt"
#Escribo los titulos al archivo donde van a quedar los resultados
#atencion que si ya existe el archivo, esta instruccion LO SOBREESCRIBE, y lo que estaba antes se pierde
#la forma que no suceda lo anterior es con append=TRUE
cat( file=archivo_salida,
sep= "",
"max_depth", "\t",
"min_split", "\t",
"ganancia_promedio", "\n")
#itero por los loops anidados para cada hiperparametro
#Aqui usted debera agregar loops !
curr_iter=0
for( vcp in  c( -1,-0.5, 0, 0.1 ) ){
for( vmax_depth  in  c( 4, 6, 8, 10, 12, 14 )  ) {
for( vmin_split  in  c( 1000, 800, 600, 400, 200, 100, 50, 20, 10 )  ){
for (vmin_bucket in  c(2, 4, 8, 16, 32, vmin_split/2, vmin_split/4 )){
total_iter <- length(c(-1, -0.5, 0, 0.1)) * length(c(4, 6, 8, 10, 12, 14)) * length(c(1000, 800, 600, 400, 200, 100, 50, 20, 10)) * length(c(2, 4, 8, 16, 32, vmin_split/2, vmin_split/4))
#notar como se agrega
param_basicos  <- list( "cp"=         vcp,       #complejidad minima
"minsplit"=  vmin_split,  #minima cantidad de registros en un nodo para hacer el split
"minbucket"=  vmin_bucket,#minima cantidad de registros en una hoja
"maxdepth"=  vmax_depth ) #profundidad máxima del arbol
#Un solo llamado, con la semilla 17
ganancia_promedio  <- ArbolesMontecarlo( ksemillas,  param_basicos )
#escribo los resultados al archivo de salida
cat(  file=archivo_salida,
append= TRUE,
sep= "",
vcp, "\t",
vmin_bucket, "\t",
vmax_depth, "\t",
vmin_split, "\t",
ganancia_promedio, "\n"  )
curr_iter <- curr_iter+1
cat("Iteration ", curr_iter, " of ", total_iter, "\n")
}
}
}
}
#Ensemble de arboles de decision
#utilizando el naif metodo de Arboles Azarosos
#entreno cada arbol utilizando un subconjunto distinto de atributos del dataset
#Para detener el script y mostrar el stack ante un error
options(error = function() {
traceback(20);
options(error = NULL);
stop("\nAbortando luego de un error fatal en el script.\n")
})
#limpio la memoria
rm( list=ls() )  #Borro todos los objetos
gc()   #Garbage Collection
require("data.table")
require("rpart")
# INICIO parametros experimento
PARAM <- list()
PARAM$experimento  <- 3211
PARAM$semilla  <- 999959      #Establezco la semilla aleatoria, cambiar por SU primer semilla
#parameetros rpart
PARAM$rpart_param   <- list( "cp"=          -1,
"minsplit"=  50,
"minbucket"=  5,
"maxdepth"=   6 )
#parametros  arbol
PARAM$feature_fraction  <- 0.5  #entreno cada arbol con solo 50% de las variables variables
PARAM$num_trees_max  <- 500 #voy a generar 500 arboles, a mas arboles mas tiempo de proceso y MEJOR MODELO, pero ganancias marginales
# FIN parametros experimento
#------------------------------------------------------------------------------
#------------------------------------------------------------------------------
#Aqui comienza el programa
setwd("C:/Users/varon/OneDrive/Escritorio/Data_mining")  #Establezco el Working Directory
#cargo los datos
dataset  <- fread("./datasets/dataset_pequeno.csv")
#creo la carpeta donde va el experimento
dir.create( "./exp/", showWarnings = FALSE  )
carpeta_experimento  <-  paste0( "./exp/KA", PARAM$experimento, "/")
dir.create( paste0( "./exp/KA", PARAM$experimento, "/"),
showWarnings = FALSE )
setwd( carpeta_experimento )
#que tamanos de ensemble grabo a disco, pero siempre debo generar los 500
grabar  <- c( 1, 5, 10, 50, 100, 200, 500)
#defino los dataset de entrenamiento y aplicacion
dtrain  <- dataset[ foto_mes==202107 ]
dapply  <- dataset[ foto_mes==202109 ]
#aqui se va acumulando la probabilidad del ensemble
dapply[ , prob_acumulada := 0 ]
#Establezco cuales son los campos que puedo usar para la prediccion
#el copy() es por la Lazy Evaluation
campos_buenos  <- copy( setdiff(  colnames(dtrain) ,  c("clase_ternaria") ) )
#Genero las salidas
set.seed(PARAM$semilla) #Establezco la semilla aleatoria
for( arbolito in  1:PARAM$num_trees_max )
{
qty_campos_a_utilizar  <- as.integer( length(campos_buenos)* PARAM$feature_fraction )
campos_random  <- sample( campos_buenos, qty_campos_a_utilizar )
#paso de un vector a un string con los elementos separados por un signo de "+"
#este hace falta para la formula
campos_random  <- paste( campos_random, collapse=" + ")
#armo la formula para rpart
formulita  <- paste0( "clase_ternaria ~ ", campos_random )
#genero el arbol de decision
modelo  <- rpart( formulita,
data= dtrain,
xval= 0,
control= PARAM$rpart_param )
#aplico el modelo a los datos que no tienen clase
prediccion  <- predict( modelo, dapply , type = "prob")
dapply[  ,  prob_acumulada :=  prob_acumulada + prediccion[ , "BAJA+2"] ]
if( arbolito %in%  grabar )
{
#Genero la entrega para Kaggle
umbral_corte  <-  (1/40) *  arbolito
entrega  <- as.data.table( list( "numero_de_cliente"= dapply[  , numero_de_cliente],
"Predicted"= as.numeric(dapply[  , prob_acumulada] > umbral_corte) ) ) #genero la salida
nom_arch  <- paste0('KA', PARAM$experimento, "_",
sprintf( '%.3d', arbolito ), #para que tenga ceros adelante
'.csv' )
fwrite( entrega,
file= nom_arch,
sep= "," )
cat( arbolito, " " )
}
cat( arbolito, "\n" )  #imprimo cada pasada para no angustiarme
}
#Ensemble de arboles de decision
#utilizando el naif metodo de Arboles Azarosos
#entreno cada arbol utilizando un subconjunto distinto de atributos del dataset
#Para detener el script y mostrar el stack ante un error
options(error = function() {
traceback(20);
options(error = NULL);
stop("\nAbortando luego de un error fatal en el script.\n")
})
#limpio la memoria
rm( list=ls() )  #Borro todos los objetos
gc()   #Garbage Collection
require("data.table")
require("rpart")
# INICIO parametros experimento
PARAM <- list()
PARAM$experimento  <- 3213
PARAM$semilla  <- 999959      #Establezco la semilla aleatoria, cambiar por SU primer semilla
#parameetros rpart
PARAM$rpart_param   <- list( "cp"=          -1,
"minsplit"=  250,
"minbucket"=  5,
"maxdepth"=   8 )
#parametros  arbol
PARAM$feature_fraction  <- 0.5  #entreno cada arbol con solo 50% de las variables variables
PARAM$num_trees_max  <- 500 #voy a generar 500 arboles, a mas arboles mas tiempo de proceso y MEJOR MODELO, pero ganancias marginales
# FIN parametros experimento
#------------------------------------------------------------------------------
#------------------------------------------------------------------------------
#Aqui comienza el programa
setwd("C:/Users/varon/OneDrive/Escritorio/Data_mining")  #Establezco el Working Directory
#cargo los datos
dataset  <- fread("./datasets/dataset_pequeno.csv")
#creo la carpeta donde va el experimento
dir.create( "./exp/", showWarnings = FALSE  )
carpeta_experimento  <-  paste0( "./exp/KA", PARAM$experimento, "/")
dir.create( paste0( "./exp/KA", PARAM$experimento, "/"),
showWarnings = FALSE )
setwd( carpeta_experimento )
#que tamanos de ensemble grabo a disco, pero siempre debo generar los 500
grabar  <- c( 1, 5, 10, 50, 100, 200, 500)
#defino los dataset de entrenamiento y aplicacion
dtrain  <- dataset[ foto_mes==202107 ]
dapply  <- dataset[ foto_mes==202109 ]
#aqui se va acumulando la probabilidad del ensemble
dapply[ , prob_acumulada := 0 ]
#Establezco cuales son los campos que puedo usar para la prediccion
#el copy() es por la Lazy Evaluation
campos_buenos  <- copy( setdiff(  colnames(dtrain) ,  c("clase_ternaria") ) )
#Genero las salidas
set.seed(PARAM$semilla) #Establezco la semilla aleatoria
for( arbolito in  1:PARAM$num_trees_max )
{
qty_campos_a_utilizar  <- as.integer( length(campos_buenos)* PARAM$feature_fraction )
campos_random  <- sample( campos_buenos, qty_campos_a_utilizar )
#paso de un vector a un string con los elementos separados por un signo de "+"
#este hace falta para la formula
campos_random  <- paste( campos_random, collapse=" + ")
#armo la formula para rpart
formulita  <- paste0( "clase_ternaria ~ ", campos_random )
#genero el arbol de decision
modelo  <- rpart( formulita,
data= dtrain,
xval= 0,
control= PARAM$rpart_param )
#aplico el modelo a los datos que no tienen clase
prediccion  <- predict( modelo, dapply , type = "prob")
dapply[  ,  prob_acumulada :=  prob_acumulada + prediccion[ , "BAJA+2"] ]
if( arbolito %in%  grabar )
{
#Genero la entrega para Kaggle
umbral_corte  <-  (1/40) *  arbolito
entrega  <- as.data.table( list( "numero_de_cliente"= dapply[  , numero_de_cliente],
"Predicted"= as.numeric(dapply[  , prob_acumulada] > umbral_corte) ) ) #genero la salida
nom_arch  <- paste0('KA', PARAM$experimento, "_",
sprintf( '%.3d', arbolito ), #para que tenga ceros adelante
'.csv' )
fwrite( entrega,
file= nom_arch,
sep= "," )
cat( arbolito, " " )
}
cat( arbolito, "\n" )  #imprimo cada pasada para no angustiarme
}
